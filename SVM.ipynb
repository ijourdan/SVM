{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Support Vector Machine (SVM)\n",
    "==\n",
    "\n",
    "\n",
    "\n",
    "Temas a desarrollar:\n",
    "\n",
    "* Reglas de decisión, o mejor, Cotas de decisión\n",
    "* Aproximación por la calle más ancha\n",
    "* Kernels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Repaso : Multiplicadores de Lagrange\n",
    "\n",
    "### Motivación\n",
    "El método de multiplicadores de Lagrange, es uno de los métodos más utilizados para optimización sujeta a restricciones.\n",
    "\n",
    "<img src=\"imgs/lagrange2d.gif\" width=300 height=300 align=\"left\"/>\n",
    "Supongamos que se busca encontrar el máximo (o el mínimo) de una función $f(\\mathbf{x})$ con $\\mathbf{x}=[x_1, ..., x_n]^T$ (ver la figura), entonces debemos encontrar $\\mathbf{x}_s$ tal que se cumpla\n",
    "\n",
    "$$ \\nabla_\\mathbf{x} f(\\mathbf{x}_s) = 0$$\n",
    "\n",
    "\n",
    "Ahora, bien encontrar el valor de la solución $\\mathbf{x}_s$ no es simple. Se puede partir desde una coordenada $\\mathbf{x}_0$ y avanzar hasta encontrar en forma exploratoria a $\\mathbf{x}_s$. Este método no es muy eficiente. Una mejor solución es emplear el método del gradiente descendente hasta llegar a la solución. \n",
    "\n",
    "Pero, si hubiera alguna condición que se deba cumplir, es decir, alguna restricción sobre los valores que pudiera tomar $\\mathbf{x}_s$, por ejemplo, $g_1(\\mathbf{x}_s) = c_1$ con $c_1=cte$, entonces los caminos para buscar el mínimo se reducen, ya que, el camino está definido por la condición (curva azul en la figura). En el caso de la condición de la figura encontraremos dos máximos y un mínimo. \n",
    "\n",
    "Siguiendo el ejemplo de la figura, los máximos y mínimos que se determinan no son los absolutos de la función, son máximos y mínimos de la función bajo las restricciones que impone $g_1(\\mathbf{x})=c_1$.\n",
    "\n",
    "Si se agregaría una condición más, $g_2(\\mathbf{x})=c_2$, entonces los puntos $\\mathbf{x}_s$ a determinar se encontrarán en la intersección entre las dos condiciones. Se imaginar 4 puntos de intersección en el ejemplo. Entonces la decisión sobre cuál es el máximo y mínimo se buscará entre esos 4 puntos. Es decir, cada condición implica una pérdida de dimensión.  \n",
    "\n",
    "### Método de multiplicadores de Lagrange\n",
    "\n",
    "#### Enunciado.\n",
    "\n",
    "Si una función $f:\\mathbb{R}^n \\rightarrow \\mathbb{R} : f(x_1, x_2, ..., x_n)$ tiene un extremo relativo cuando está sometida a $m$ restricciones:\n",
    "\n",
    "$$g_1(x_1, ..., x_n)=0, ..., g_m(x_1, ..., x_m)=0 \\label{eq:restricciones} $$\n",
    "\n",
    "siendo $m<n$, existen entonces $m$ escalares $\\lambda_1, \\lambda_2, ..., \\lambda_m$ tales que:\n",
    "\n",
    "$$ \\nabla f = \\lambda_1 \\nabla g_1 + \\lambda_2 \\nabla g_2 + ...+ \\lambda_m g_m $$\n",
    "\n",
    "#### ¿Qué significa?\n",
    "\n",
    "No se va a demostrar el método de multiplicadores de Lagrange, no obstante se exponen algunas pautas sobre su aplicación, principalmente a optimización.\n",
    "\n",
    "Si el problema busca determinar los extremos de la función $f(\\mathbf{x})$, con $\\mathbf{x} =[x_1, x_2, ..., x_n]^T$, en un problema configurado con restricciones, de acuerdo a la ec. \\ref{eq:restricciones}, podemos definir una función $\\mathcal{L}:\\mathbb{R}^n \\rightarrow \\mathbb{R} : \\mathcal{L}(\\mathbf{x})$ como:\n",
    "\n",
    "$$\\mathcal{L}(x_1, ..., x_n, \\lambda_1, ..., \\lambda_m) = f(\\mathbf{x}) + \\sum_{i=1}^{m}{\\lambda_i g_i(\\mathbf{x})}$$\n",
    "\n",
    "donde $f(\\mathbf{x}) = \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda})$ para aquellos $\\mathbf{x}  ~|~  g_i(\\mathbf{x})=0 ~ \\forall ~  i\\in \\{1, ..., m\\}$. Luego, buscamos los extremos de $\\mathcal{L}$, que serán los extremos de $f$ restringidos por $g_i$:\n",
    "\n",
    "$$ \\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) = \\nabla f(\\mathbf{x}) + \\sum_{i=1}^{m}{\\lambda_i \\nabla g_i(\\mathbf{x})} = 0 $$\n",
    "\n",
    "$$ \\nabla_{\\mathbf{\\lambda}} \\mathcal{L}(\\mathbf{x}, \\mathbf{\\lambda}) =  [g_1(\\mathbf{x}), ... ,g_m(\\mathbf{x})]^T = [0, ..., 0]^T $$\n",
    "\n",
    "Las ecuaciones anteriores implican un sistema de  $n+m$ ecuaciones con $n+m$ incógnitas. La resolución se simplifica en la medida que $m$ sea tal que reduzca los grados de libertad del conjunto solución.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "### Motivación\n",
    "\n",
    "<img src=\"imgs/01.png\" width=300 height=300 align=\"left\"/> <img src=\"imgs/02.png\" width=300 height=300 align=\"left\"/>  El problema de clasificación es encontrar el hiperespacio que permita separar muestras que pertenecen a dos clases diferentes. Los algoritmos, como el perceptrón, tanto el simple como el multicapa, ofrecen una solución, dentro de muchas posibles (ver figura izquierda). Algunas soluciones no son tan buenas como otras, por ejemplo, un hiperplano muy cerca de las muestras es muy propenso a clasificar incorrectamente. ¿Entonces, cuál de todos los posibles hiperplanos es el mejor? Support Vector Machine (SVM) busca de todas las soluciones, el hiperplano que proponga la **calle** más ancha (ver figura derecha). La **calle** queda definida por rectas paralelas al hiperplano que contienen las muestras más cercanas al hiperplano. Esas muestras se denominan el **soporte**.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Condiciones fundamentales.\n",
    "\n",
    "### Regla de decisión\n",
    "\n",
    "<img src=\"imgs/03.png\" width=300 height=300 align=\"left\"/> El hiperplano de separación está definido, como siempre, por un vector ortogonal, en este caso, $\\bar{w} \\perp hiperplano$. A partir de este vector, podemos clasificar las puntos del espacio, como pertenecientes a una clase u otra. Para cualquier vector $\\bar{u}$, se puede determina que se encuentra en un lado (+) del hiperplano si se cumple:\n",
    "\n",
    "$$ \\bar{w} \\cdot \\bar{u} \\leq c $$\n",
    "\n",
    "donde $(\\cdot)$ es el producto escalar y $c$ es el parámetro que no dice cuándo la clase es (+), o negativa (-) en caso de ser $\\bar{w} \\cdot \\bar{u} < c$. \n",
    "\n",
    "Sin perder generalidad, se puede definir la **Regla de decisión**.\n",
    "\n",
    "$$\\bar{w} \\cdot \\bar{u} + b \\leq 0 ~ \\implies ~ (+)\\label{eq:I}$$\n",
    "\n",
    "donde $ b = -c $. No se sabe todavía quién es $\\bar{w}$ ni $b$, pero sí se conoce que $\\bar{w}$ es ortogonal al hiperplano. \n",
    "\n",
    "$$$$\n",
    "\n",
    "### Definición de la Calle\n",
    "\n",
    " Para construir la **calle** se puede definir que, siendo $\\bar{x}_+$ y $\\bar{x}_-$ muestras de entrenamiento de clases $(+)$ y $(-)$ respectivamente.Se define entonces:\n",
    "\n",
    "$$ \\bar w \\cdot \\bar x_+ + b \\geq 1$$\n",
    "$$ \\bar w \\cdot \\bar x_- + b \\leq (-1)$$\n",
    "\n",
    "Ahora bien, para simplificar la matemática, se introduce una nueva variable, $y_i$ tal que \n",
    "\n",
    "$$ y_i = 1 ~~~ para ~~~ (+)$$\n",
    "$$ y_i = -1 ~~~ para ~~~ (-)$$\n",
    "\n",
    "luego, multiplicamos a ambos lados de la ecuación:\n",
    "\n",
    "$$y_i \\left ( \\bar w \\cdot \\bar x_+ + b \\right ) \\geq y_i ~  1$$\n",
    "$$y_i \\left ( \\bar w \\cdot \\bar x_- + b \\right ) \\leq y_i ~ (-1)$$\n",
    "\n",
    "<img src=\"imgs/04.png\" width=300 height=300 align=\"left\"/> Se puede reescribir esta regla como una única ecuación:\n",
    "\n",
    "$$y_i \\left ( \\bar w \\cdot \\bar x + b \\right ) \\geq 1$$\n",
    "\n",
    "donde la regla de decisión se realiza en base a que $y_i$ sea 1 o -1. También se puede escribir:\n",
    "\n",
    "$$y_i \\left ( \\bar w \\cdot \\bar x + b \\right ) - 1 \\geq 0$$\n",
    "\n",
    "Y se obtiene finalmente la condición que deben cumplir las muestras que definen la **calle** o el **soporte** del hiperplano \n",
    "\n",
    "$$y_i \\left ( \\bar w \\cdot \\bar x + b \\right ) - 1 = 0 ~~~ \\implies ~~~ \\bar x ~~~ en ~la~ calle \\label{eq:II}$$\n",
    "\n",
    "Quedan definidas, entonces, las cotas de decisión, según las (ec. \\ref{eq:I}) y (ec. \\ref{eq:II})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ancho de la calle\n",
    "\n",
    "<img src=\"imgs/05.png\" width=300 height=300 align=\"left\"/>  Ahora se buscará que la calle sea la más ancha posible.\n",
    "\n",
    "$$\\bar D = \\bar x_+ - \\bar x_-$$\n",
    "\n",
    "$$ A = \\bar D \\cdot \\frac{\\bar w}{\\| \\bar w \\|}$$\n",
    "\n",
    "$$ A = \\left ( \\bar x_+ - \\bar x_- \\right ) \\cdot \\frac{\\bar w}{ \\| \\bar w \\|}$$\n",
    "\n",
    "pero, considerando (ec. \\ref{eq:II}), donde $y_i = 1$ con $\\bar x_+$ e $y_i = -1 $ con $\\bar x_-$, se tiene\n",
    "\n",
    "$$ \\bar x_+ \\cdot \\bar w = 1 - b$$\n",
    "\n",
    "$$ - \\bar x_- \\cdot \\bar w = 1 + b$$\n",
    "\n",
    "por lo tanto\n",
    "\n",
    "$$A = \\left ( 1 - b + 1 + b \\right ) \\frac{1}{\\| \\bar w \\|} = \\frac{2}{\\| w \\|}$$\n",
    "\n",
    "y se quiere que $A = \\max_{\\bar w} \\left ( \\frac{2}{\\| \\bar w \\| } \\right )$ y eso depende de $\\bar w$. Encontrar el máximo valor de $A$ equivale a encontrar el mínimo valor de $\\| \\bar w \\|$, o bien:\n",
    "\n",
    "$$ \\bar w = \\min_{\\bar w} \\left ( \\frac{1}{2} \\| \\bar w \\| ^2 \\right ) \\label{eq:III} $$\n",
    "\n",
    "**Entonces se debe resolver la ecuación \\ref{eq:III}, de acuerdo a las condiciones que impone (ec. \\ref{eq:II}).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución \n",
    "\n",
    "Para encontrar la solución a la ecuación \\ref{eq:III}, bajo las restricciones de la ec. \\ref{eq:II}, se puede aplicar el *Método de Multiplicadores de Lagrange*, para ello definimos (ver el repaso):\n",
    "\n",
    "$$ \\mathcal{L}(\\bar w, b) = \\frac{1}{2} \\| \\bar w \\|^2 - \\sum_i \\alpha_i \\left( y_i (\\bar w \\cdot \\bar x_i + b)-1\\right) ~~~~con~\\alpha_i \\geq 0$$\n",
    "\n",
    "Una cuestión importante es que $\\mathcal{L}(\\bar w,b)$ es convexa. Por lo tanto el mínimo se encuentra a partir de las derivadas parciales:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}(\\bar w, b)}{\\partial \\bar w} = \\bar w - \\sum_i \\alpha_i y_i \\bar x_i = \\bar 0$$\n",
    "\n",
    "se obtiene entonces:\n",
    "\n",
    "$$\\bar w = \\sum_i \\alpha_i y_i \\bar x_i \\label{eq:IV} $$\n",
    "\n",
    "Esta última ecuación es importante, ya que no dice que $\\bar w$ es una suma lineal de muestras $\\bar x_i$, pero no de todas, sino de aquellas para las cuales $\\alpha_i \\not = 0$.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\bar w, b)}{\\partial b} = \\sum_i \\alpha_i y_i = 0 \\label{eq:V}$$\n",
    "\n",
    "esta última ecuación no servirá más adelante.\n",
    "\n",
    "### Condiciones complementarias. Definición del Soporte.\n",
    "\n",
    "Las restricciones que plante la ec.\\ref{eq:II}, son condiciones complementarias del método de optimización, y es interesante ver qué aportan. Cada término restando de $\\mathcal{L}(\\hat w, b)$ indica que\n",
    "\n",
    "$$ \\forall i ~~~\\alpha_i \\left ( y_i (\\bar w \\cdot \\bar x_i +b) - 1 \\right ) = 0 $$\n",
    "\n",
    "eso implica\n",
    "\n",
    "$$ \\alpha_i = 0 ~~~~~~~\\lor ~~~~~~~ y_i (\\bar w \\cdot \\bar x_i +b) - 1 = 0$$\n",
    "\n",
    "considerando la ec. \\ref{eq:IV}, los términos que aportan son aquellos para los cuales $\\alpha_i \\not = 0$, y por lo tanto los $\\hat x_i $ que aportan a la definición de $\\bar w$ son aquellos que cumplen:\n",
    "\n",
    "$$ y_i (\\bar w \\cdot \\bar x_i +b) = 1 $$\n",
    "\n",
    "Tales vectores se llaman **vectores de soporte**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html\n",
    "\n",
    "def ap_svm(X,y): # fit the model, don't regularize for illustration purposes\n",
    "    # Parte relevante. SVC: Support Vector Classification.\n",
    "    # Parmetro C: parámetro de regularización. Cuanto más grande menos muestras mal clasificadas se permiten\n",
    "    #             durante el aprendizaje. C grande tiende a overfeating. C pequeños, pero no muy chicos, \n",
    "    #             generalizan bien.\n",
    "    clf = svm.SVC(kernel='linear', C=1)\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)  #<===== hiperplano de separación\n",
    "   \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create 40 separable points\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=6)\n",
    "ap_svm(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XOR:\n",
    "N=100\n",
    "# Función XOR\n",
    "xor_x = np.asarray([[-1,-1],[-1,1],[1,-1],[1,1]] )\n",
    "xor_y = np.asarray([-1,1,1,-1])\n",
    "aux = np.random.rand(N,2)*15+5\n",
    "aux2 = np.random.choice(4,N)\n",
    "XX=[]\n",
    "y=[]\n",
    "for i in range(len(aux2)):\n",
    "    XX.append(xor_x[aux2[i]]*aux[i])\n",
    "    y.append(xor_y[aux2[i]])\n",
    "    \n",
    "XOR_XX = np.asarray(XX)\n",
    "XOR_y = np.asarray(y)\n",
    "\n",
    "\n",
    "ap_svm(XOR_XX,XOR_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de optimización dual\n",
    "\n",
    "SVM, planteado de esta manera, en principio posee un problema común con el perceptrón. Separa por hiperplanos, y por lo tanto no puede separar conjuntos que no sean linealmente separables. No obstante, con un poco de magia en la matemática, se pueden encontrar soluciones para tratar con este problema. De esto se tratan los Kernels.\n",
    "\n",
    "Volviendo a considerar la solución dada por la ec. \\ref{eq:IV}: $\\bar w = \\sum_i \\alpha_i y_i \\bar x_i$, podemos reescribir $\\mathcal{L}(\\bar w, b)$ \n",
    "\n",
    "$$ \\mathcal{L}(\\bar w, b) = \\frac{1}{2} \\| \\bar w \\|^2 - \\sum_i \\alpha_i \\left( y_i (\\bar w \\cdot \\bar x_i + b)-1\\right) ~~~~con~\\alpha_i \\geq 0$$\n",
    "\n",
    "como:\n",
    "\n",
    "$$\\mathcal{L}(\\alpha_i,b) = \\frac{1}{2} \\left ( \\sum_i \\alpha_i y_i \\bar x_i \\right ) \\cdot \\left ( \\sum_j \\alpha_j y_j \\bar x_j \\right ) - \\sum_i \\alpha_i  y_i \\bar x_i \\cdot \\left ( \\sum_j \\alpha_j y_j \\bar x_j \\right ) - \\sum_i \\alpha_i y_i b - \\sum_i \\alpha_i ~~~~con~\\alpha_i \\geq 0$$\n",
    "\n",
    "incluyendo además la condición que surge en la búsqueda del óptimo, ec. \\ref{eq:V}, se puede concluir:\n",
    "\n",
    "$$\\mathcal{L}(\\alpha_i,b) = \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j ~\\alpha_i~\\alpha_j~ y_i~y_j ~ (\\bar x_i \\cdot \\bar x_j )$$\n",
    "\n",
    "bajo la condición $\\alpha_i \\geq 0$ y $\\sum_i \\alpha_i y_i =0$.\n",
    "\n",
    "Se puede observar que $\\mathcal{L}(\\alpha_i, b)$ depende linealmente del producto interno $(\\bar x_i \\cdot \\bar x_j )$, y no directamente de $\\bar x_i$ y $\\bar x_j$. \n",
    "\n",
    "Volviendo a la ec. \\ref{eq:I} sumando el resultado obtenido sobre el $\\bar w$ óptimo, en la ec. \\ref{eq:IV}, se puede reescribir la regla de decisión:\n",
    "\n",
    "$$ \\sum_i \\alpha_i y_i (\\bar x_i \\cdot \\bar u ) + b \\geq 0 ~~~~ \\implies ~~~~ (+) \\label{eq:VI}$$\n",
    "\n",
    "La regla de decisión de la ec. \\ref{eq:VI} depende linealmente del producto interno $(\\bar x_i \\cdot \\bar u )$, siendo $\\bar u$ un elemento que se busca clasificar.\n",
    "\n",
    "\n",
    "**NOTA: La clasificación depende linealmente del producto interno entre los vectores, es decir, depende linealmente de relaciones geométricas.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels en SVM\n",
    "\n",
    "### Existencia de una solución.\n",
    "\n",
    "<img src=\"imgs/06.png\" width=500 align=\"center\"/> \n",
    "\n",
    "La función **XOR** es interesante como ejemplo, ya que no hay una forma de separar la clase **(+): 1** de la clase **(-): -1** (ver figura). La optimización dual determina una regla de decisión, ec. \\ref{eq:VI}, donde importa la relación geométrica en la clasificación. Por lo tanto, puede pensarse en definir una función:\n",
    "\n",
    "$$\\Phi : \\mathbb{X} \\longrightarrow \\mathfrak{h}$$\n",
    "\n",
    "donde $\\bar x, \\bar u \\in \\mathbb{X}$ y $\\mathfrak{h}$ es espacio de Hilbert (a los fines prácticos, un espacio con producto interno). \n",
    "Mediante la función $\\Phi(\\bar u)$ la muestra a clasificar $\\bar u$ se \"*mapea*\" en un espacio de Hilber $\\mathfrak{h}$ de mayor dimensión, posiblemente de dimensión infinita. En este caso, la regla de decisión será,\n",
    "\n",
    "$$ \\sum_i \\alpha_i y_i (\\Phi(\\bar x_i) \\cdot \\Phi(\\bar u) ) + b \\geq 0 ~~~~ \\implies ~~~~ (+) \\label{eq:VI}$$\n",
    "\n",
    "Entonces, encontrando una función $\\Phi$ apropiada, algo que se abordará en breve, es posible obtener una solución. Este pase mágico posee un costo asociado. \n",
    "\n",
    "El problema de optimización empleando un producto interno $(\\bar x \\cdot \\bar{x}^,)$ se resuelve en $O(n)$, pero al aplicar  $\\Phi : \\mathbb{X} \\longrightarrow \\mathfrak{H}$, existe un incremento en la dimensionalidad, el problema se resuleve en $O\\left(dim(\\mathfrak{h})\\right)$, donde en general\n",
    "\n",
    "$$O\\left(dim(\\mathfrak{h})\\right) \\gg O(n)$$\n",
    "\n",
    "con la posibilidad de $dim(\\mathfrak{h})=\\infty $. De todas formas, nuevamente, no importa realmente la función $\\Phi$, importa el producto interno a partir de la función $\\Phi$: \n",
    "\n",
    "$$ K(\\bar x, \\bar x ^,) = \\Phi(\\bar x) \\cdot \\Phi(\\bar x ^,)$$ \n",
    "\n",
    "que debe cumplir, en principio, ser un productor interno. Esta estructura es lo que definimos como **kernel**.\n",
    "\n",
    "### Definición de Kernel.\n",
    "\n",
    "Una función $K: \\mathbb{X} \\times \\mathbb{X} \\longrightarrow \\mathbb{R}$ es un **kernel** sobre $\\mathbb{X}$, si existe algún mapeo $\\Phi : \\mathbb{X} \\longrightarrow \\mathfrak{h}$, donde $\\mathfrak{h}$ es un espacio de Hilber, llamado también \"*feature space*\", tal que:\n",
    "\n",
    "$$ \\forall \\bar x, \\bar x ^, \\in \\mathbb{X} ~,~ K(\\bar x , \\bar x ^,) = \\left (\\Phi(\\bar x) \\cdot \\Phi(\\bar x ^,) \\right )$$\n",
    "\n",
    "Algunas consideraciones:\n",
    "\n",
    "- No importa $\\Phi(\\bar x)$ en sí, ni importa encontrarla. Interesa que exista. Encontrarla, y sobre todo, aplicarla, es contraproducente desde el punto de vista computacional. \n",
    "- El kernel $K$ puede ser arbitrario. Es suficiente con que exista $Phi$.\n",
    "\n",
    "para ello es suficiente que se cumpla el **Teorema de Mercer**. No se va a profundizar sobre el cumplimiento de este teorema, pero una consecuencia directa es que **el kernel $K(\\bar x , \\bar x ^,)$ sea definido no-negativo**.\n",
    "\n",
    "#### Kernel simétrico definido no-negativo.\n",
    "\n",
    "Un kernel $K: \\mathbb{X} \\times \\mathbb{X} \\longrightarrow \\mathbb{R}$ se dice simétrico definido no-negativo si\n",
    "\n",
    "$$ \\forall \\left \\{ x_1, x_2, ... , x_m \\right \\} \\subseteq \\mathbb{X}, ~~la~matriz~~ {\\bar K} = \\left [ K(x_i , x_j) \\right ] \\in \\mathbb{R}^{m \\times m}  $$\n",
    "\n",
    "es simétrica definida no-negativa, es decir,\n",
    "\n",
    "$$  \\bar K ~~ = ~~{ \\bar K} ^ T$$\n",
    "\n",
    "$$ \\forall c \\in \\mathbb{R}^m ~~~~c^T \\bar K c \\geq 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Kernel Polinómico. Resolución XOR\n",
    "\n",
    "\n",
    "Para una constante $c>0$, un kernel polinómico $K$ de grado $p \\in \\mathbb{N}$ se define como\n",
    "\n",
    "$$ \\forall ~ \\bar x , \\bar x ^, \\in \\mathbb{R}, ~~ K(\\bar x , \\bar x ^,) = (\\bar x \\cdot \\bar x ^, + c)^d $$ \n",
    "\n",
    "donde, para el ejemplo, $\\bar x = [x_1 , x_2]^T $ y $\\bar x ^, = [x^,_1 , x^,_2]^T $.\n",
    "\n",
    "Desarrollando para el ejemplo bidimensional, el kernel tiene la forma:\n",
    "\n",
    "$$ K(\\bar x , \\bar x ^,) =( x_1 x_1^, + x_2 x_2^, + c)^2 $$\n",
    "\n",
    "$$ K(\\bar x , \\bar x ^,) = x_1^2 {x_1^,}^2 + x_2^2 {x_2^,}^2 + c^2 + \\sqrt{2}x_1 x_2 ~ \\sqrt{2}~x_1^, x_2^, + \\sqrt{2c}~x_1 ~ \\sqrt{2c}~x_1^, + \\sqrt{2c}~x_2 ~ \\sqrt{2c}~x_2^, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ K(\\bar x , \\bar x ^, ) =  \\begin{pmatrix} {x_1}^2  \\\\ {x_2}^2  \\\\ c \\\\ \\sqrt{2}~x_1 x_2 \\\\ \\sqrt{2c}~x_1 \\\\ \\sqrt{2c}~x_2 \\end{pmatrix} \\cdot \\begin{pmatrix} {x_1^,}^2  \\\\ {x_2^,}^2  \\\\ c \\\\ \\sqrt{2}~x_1^, x_2^, \\\\\\sqrt{2c}~x_1^,\\\\  \\sqrt{2c}~x_2^, \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "donde es clara la existencia de una función de  mapeo $\\Phi: \\mathbb{R}^2 \\longrightarrow \\mathbb{R}^6 $:\n",
    "\n",
    "$$ \\Phi \\begin{pmatrix} x_1  \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} {x_1}^2  \\\\ {x_2}^2  \\\\ c \\\\ \\sqrt{2}~x_1 x_2 \\\\\\sqrt{2c}~x_1\\\\  \\sqrt{2c}~x_2 \\end{pmatrix} \\label{eq:VII}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Resolución:\n",
    "\n",
    "La función $XOR$ se puede expresar mediante la siguiente clasificación\n",
    "\n",
    "$$ \\left \\{ \\begin{pmatrix} 1  \\\\ -1 \\end{pmatrix}, \\begin{pmatrix} -1  \\\\ 1 \\end{pmatrix} \\right \\} ~~\\longrightarrow y = 1~~~~~\\left \\{ \\begin{pmatrix} 1  \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} -1  \\\\ -1 \\end{pmatrix} \\right \\} ~~\\longrightarrow y = -1$$\n",
    "\n",
    "<img src=\"imgs/07.png\" width=300 align=\"left\"/> \n",
    "El mapeo mediante dado por la función $\\Phi$ (ec. \\ref{eq:VII}) determina la siguiente clasificación en el espacio de features:\n",
    "\n",
    "$$ \\left \\{ \\begin{pmatrix} 1  \\\\ 1 \\\\ 1 \\\\ - \\sqrt 2 \\\\ \\sqrt 2 \\\\ - \\sqrt 2 \\end{pmatrix}, \\begin{pmatrix} 1  \\\\ 1 \\\\ 1 \\\\ - \\sqrt 2 \\\\ - \\sqrt 2 \\\\ \\sqrt 2 \\end{pmatrix} \\right \\} ~~\\longrightarrow y = 1 ~~~~~ \\left \\{ \\begin{pmatrix} 1  \\\\ 1 \\\\ 1 \\\\ \\sqrt 2 \\\\ \\sqrt 2 \\\\ \\sqrt 2 \\end{pmatrix}, \\begin{pmatrix} 1  \\\\ 1 \\\\ 1 \\\\ \\sqrt 2 \\\\ - \\sqrt 2 \\\\ -\\sqrt 2 \\end{pmatrix} \\right \\} ~~\\longrightarrow y = -1$$\n",
    "\n",
    "\n",
    "Entonces, observando las coordenadas 5 y 6, se puede observar que el problema es ahora linealmente separable, y por lo tanto se puede resolver (ver figura).\n",
    "\n",
    "Además, **todos los puntos son soporte**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap2_svm(X,y,kernel='poly',C=.1):\n",
    "    \n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    clf = svm.SVC(kernel=kernel, degree=4, C=C).fit(X, y)\n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    # plot support vectors\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "               linewidth=1, facecolors='none', edgecolors='k')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap2_svm(XOR_XX,XOR_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ. \n",
    "Realice este ejemplo sin el agregado de ruido que tiene la función **XOR** y compare con los resultados teóricos.\n",
    "\n",
    "## Ejemplo 2: \n",
    "En el siguiente ejemplo se emplea un **kernel de funciones de base radial (RBF)**, el cual se define como:\n",
    "\n",
    "\n",
    "$$ K(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}}{2\\sigma ^{2}}}\\right)$$\n",
    "\n",
    "Esta clase de kernels son empleados para  clasificaciones no lineales, cuando se estima que la separación se da por curvaturas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Problema plantea generar un clasificador que determine si un vector posee norma mayor o igual a 1.\n",
    "N=100\n",
    "XX = np.random.randn(N,2)\n",
    "y=[]\n",
    "for i in range(len(XX)):\n",
    "    if XX[i,0]**2 + XX[i,1]**2 < 1:\n",
    "        y.append(-1)\n",
    "    else:\n",
    "        y.append(1)\n",
    "y = np.asarray(y)\n",
    "\n",
    "\n",
    "ap2_svm(XX,y,kernel='rbf',C=10) # Notar la importancia del parámetro C."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": true,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
